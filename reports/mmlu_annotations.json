{
  "business_ethics 5": [
    "input-and-label contamination",
    0.9084593254575504
  ],
  "business_ethics 6": [
    "input contamination",
    0.8619365399143792
  ],
  "business_ethics 10": [
    "input-and-label contamination",
    0.9999745973682873
  ],
  "business_ethics 55": [
    "clean",
    0.25
  ],
  "business_ethics 56": [
    "clean",
    0.25083705357142855
  ],
  "business_ethics 61": [
    "clean",
    0.08333333333333333
  ],
  "business_ethics 75": [
    "input-and-label contamination",
    0.7683402705515089
  ],
  "business_ethics 78": [
    "input-and-label contamination",
    0.9983125
  ],
  "business_ethics 95": [
    "input-and-label contamination",
    0.8152220507544582
  ],
  "security_studies 19": [
    "clean",
    0.08701814058956915
  ],
  "security_studies 27": [
    "clean",
    0.10933048433048434
  ],
  "security_studies 28": [
    "input-and-label contamination",
    0.9571105857630681
  ],
  "security_studies 32": [
    "clean",
    0.09017478813559322
  ],
  "security_studies 55": [
    "clean",
    0.08642934196332254
  ],
  "security_studies 56": [
    "clean",
    0.23859212802768165
  ],
  "security_studies 92": [
    "clean",
    0.07158087115443192
  ],
  "security_studies 126": [
    "clean",
    0.6360887789459218
  ],
  "security_studies 134": [
    "clean",
    0.06521739130434782
  ],
  "security_studies 137": [
    "clean",
    0.6303785780240074
  ],
  "security_studies 162": [
    "input-and-label contamination",
    0.9785876603788762
  ],
  "security_studies 173": [
    "clean",
    0.12930449122126295
  ],
  "security_studies 203": [
    "clean",
    0.6738292011019283
  ],
  "security_studies 204": [
    "clean",
    0.09991496598639457
  ],
  "security_studies 231": [
    "input-and-label contamination",
    0.7167525052900546
  ],
  "security_studies 240": [
    "clean",
    0.06310418904403867
  ],
  "security_studies 243": [
    "input-and-label contamination",
    0.999898229187869
  ],
  "high_school_us_history 22": [
    "clean",
    0.4144736842105263
  ],
  "high_school_us_history 195": [
    "clean",
    0
  ],
  "moral_disputes 14": [
    "input-and-label contamination",
    0.9581298828125
  ],
  "moral_disputes 24": [
    "input contamination",
    0.8833333333333334
  ],
  "moral_disputes 29": [
    "clean",
    0.1
  ],
  "moral_disputes 35": [
    "input-and-label contamination",
    0.9945130315500685
  ],
  "moral_disputes 55": [
    "input-and-label contamination",
    0.9967041015625
  ],
  "moral_disputes 61": [
    "input-and-label contamination",
    0.8323456790123457
  ],
  "moral_disputes 69": [
    "input-and-label contamination",
    0.9999530428249437
  ],
  "moral_disputes 90": [
    "input-and-label contamination",
    0.8249023271615425
  ],
  "moral_disputes 93": [
    "input-and-label contamination",
    0.9541331684188828
  ],
  "moral_disputes 100": [
    "clean",
    0.07692307692307693
  ],
  "moral_disputes 143": [
    "input-and-label contamination",
    0.9590589517835201
  ],
  "moral_disputes 154": [
    "input-and-label contamination",
    0.8136924803591469
  ],
  "moral_disputes 223": [
    "clean",
    0.10714285714285714
  ],
  "moral_disputes 239": [
    "clean",
    0.234375
  ],
  "moral_disputes 248": [
    "input-and-label contamination",
    0.9175803402646503
  ],
  "moral_disputes 259": [
    "input-and-label contamination",
    0.996
  ],
  "moral_disputes 269": [
    "clean",
    0.27053571428571427
  ],
  "moral_disputes 277": [
    "input-and-label contamination",
    0.9518133784125896
  ],
  "moral_disputes 282": [
    "clean",
    0.3428497942386831
  ],
  "moral_disputes 285": [
    "input-and-label contamination",
    0.8125
  ],
  "moral_disputes 287": [
    "input-and-label contamination",
    0.9999530428249437
  ],
  "moral_disputes 301": [
    "input-and-label contamination",
    0.7495960295475531
  ],
  "moral_disputes 308": [
    "clean",
    0.11538461538461539
  ],
  "moral_disputes 331": [
    "input-and-label contamination",
    0.999898229187869
  ],
  "philosophy 28": [
    "input-and-label contamination",
    0.864795918367347
  ],
  "philosophy 30": [
    "input-and-label contamination",
    0.8035714285714285
  ],
  "philosophy 32": [
    "input-and-label contamination",
    0.998179335457442
  ],
  "philosophy 53": [
    "clean",
    0.11640285754307327
  ],
  "philosophy 60": [
    "clean",
    0.19859374999999999
  ],
  "philosophy 61": [
    "clean",
    0.654320987654321
  ],
  "philosophy 76": [
    "input contamination",
    0.926501205347359
  ],
  "philosophy 90": [
    "clean",
    0.36507936507936506
  ],
  "philosophy 94": [
    "input-and-label contamination",
    0.9993141289437586
  ],
  "philosophy 170": [
    "clean",
    0.25555555555555554
  ],
  "philosophy 172": [
    "input-and-label contamination",
    0.9375
  ],
  "philosophy 173": [
    "input-and-label contamination",
    0.9974373693058346
  ],
  "philosophy 175": [
    "input-and-label contamination",
    0.9195094217024042
  ],
  "philosophy 191": [
    "clean",
    0.45335276967930027
  ],
  "philosophy 230": [
    "clean",
    0.49162257495590833
  ],
  "philosophy 258": [
    "input-and-label contamination",
    0.9507738095238095
  ],
  "philosophy 260": [
    "input-and-label contamination",
    0.9956365764298799
  ],
  "philosophy 294": [
    "input-and-label contamination",
    0.9139118457300276
  ],
  "public_relations 2": [
    "clean",
    0.24338624338624337
  ],
  "public_relations 5": [
    "input-and-label contamination",
    0.9723571968469927
  ],
  "public_relations 40": [
    "clean",
    0.14307692307692307
  ],
  "public_relations 48": [
    "clean",
    0.09090909090909091
  ],
  "public_relations 52": [
    "clean",
    0.327991452991453
  ],
  "public_relations 76": [
    "input-and-label contamination",
    0.9997106481481481
  ],
  "public_relations 79": [
    "clean",
    0.2725947521865889
  ],
  "public_relations 96": [
    "clean",
    0.28615384615384615
  ],
  "public_relations 101": [
    "clean",
    0.16666666666666666
  ],
  "public_relations 104": [
    "input-and-label contamination",
    0.8254320987654321
  ],
  "public_relations 108": [
    "clean",
    0.13157894736842105
  ],
  "high_school_microeconomics 24": [
    "input-and-label contamination",
    0.9992319071461083
  ],
  "high_school_microeconomics 32": [
    "input-and-label contamination",
    0.8953203831577968
  ],
  "high_school_microeconomics 33": [
    "clean",
    0.4092548076923077
  ],
  "high_school_microeconomics 77": [
    "clean",
    0.3430769230769231
  ],
  "high_school_microeconomics 78": [
    "input-and-label contamination",
    0.9999375
  ],
  "high_school_microeconomics 100": [
    "input-and-label contamination",
    0.998179335457442
  ],
  "high_school_microeconomics 151": [
    "input-and-label contamination",
    0.9992319071461083
  ],
  "high_school_microeconomics 157": [
    "input-and-label contamination",
    0.9969947407963937
  ],
  "high_school_microeconomics 172": [
    "input-and-label contamination",
    0.9998177842565598
  ],
  "high_school_microeconomics 191": [
    "input-and-label contamination",
    0.7595947396672035
  ],
  "high_school_microeconomics 212": [
    "clean",
    0.20851818988464954
  ],
  "high_school_microeconomics 215": [
    "input-and-label contamination",
    0.8557668093679174
  ],
  "high_school_microeconomics 222": [
    "input-and-label contamination",
    0.9736328125
  ],
  "human_sexuality 9": [
    "clean",
    0.605
  ],
  "human_sexuality 58": [
    "clean",
    0.08092948717948718
  ],
  "human_sexuality 59": [
    "clean",
    0.28622448979591836
  ],
  "human_sexuality 73": [
    "clean",
    0.3966836734693877
  ],
  "human_sexuality 76": [
    "input-and-label contamination",
    0.9484650971137457
  ],
  "human_sexuality 79": [
    "input contamination",
    0.8458359821996185
  ],
  "human_sexuality 93": [
    "clean",
    0.11764705882352941
  ],
  "human_sexuality 99": [
    "clean",
    0.31125
  ],
  "human_sexuality 100": [
    "clean",
    0.6360544217687074
  ],
  "human_sexuality 118": [
    "clean",
    0.14285714285714285
  ],
  "professional_accounting 10": [
    "clean",
    0.13151041666666666
  ],
  "professional_accounting 17": [
    "clean",
    0.5384501910601618
  ],
  "professional_accounting 57": [
    "clean",
    0.521546118447379
  ],
  "professional_accounting 74": [
    "input contamination",
    0.7110327743902439
  ],
  "professional_accounting 83": [
    "input contamination",
    0.7817291011054627
  ],
  "professional_accounting 85": [
    "clean",
    0.6258325799848291
  ],
  "professional_accounting 86": [
    "input contamination",
    0.9308373917748917
  ],
  "professional_accounting 95": [
    "clean",
    0.6817774836499534
  ],
  "professional_accounting 116": [
    "input-and-label contamination",
    0.9646218039772727
  ],
  "professional_accounting 140": [
    "input-and-label contamination",
    0.9875940393518519
  ],
  "professional_accounting 152": [
    "clean",
    0.6338891820193759
  ],
  "professional_accounting 165": [
    "clean",
    0.6481261154074955
  ],
  "professional_accounting 178": [
    "clean",
    0.5432492195629551
  ],
  "professional_accounting 189": [
    "input-and-label contamination",
    0.9999977971724505
  ],
  "professional_accounting 219": [
    "input-and-label contamination",
    0.9990234375
  ],
  "professional_accounting 230": [
    "input contamination",
    0.7792063492063492
  ],
  "professional_accounting 232": [
    "input contamination",
    0.8227124183006537
  ],
  "professional_accounting 241": [
    "clean",
    0.4662261044544041
  ],
  "professional_accounting 255": [
    "input-and-label contamination",
    0.9985422740524781
  ],
  "high_school_government_and_politics 13": [
    "clean",
    0.5706521739130435
  ],
  "high_school_government_and_politics 19": [
    "input-and-label contamination",
    0.9542575650950036
  ],
  "high_school_government_and_politics 33": [
    "input-and-label contamination",
    0.9519047619047619
  ],
  "high_school_government_and_politics 37": [
    "clean",
    0.642554012345679
  ],
  "high_school_government_and_politics 40": [
    "input-and-label contamination",
    0.9953345968800117
  ],
  "high_school_government_and_politics 69": [
    "clean",
    0.1957894736842105
  ],
  "high_school_government_and_politics 75": [
    "input-and-label contamination",
    0.9998779296875
  ],
  "high_school_government_and_politics 91": [
    "input-and-label contamination",
    0.9999794989544467
  ],
  "high_school_government_and_politics 94": [
    "clean",
    0.6694812710437711
  ],
  "high_school_government_and_politics 103": [
    "clean",
    0.3689236111111111
  ],
  "high_school_government_and_politics 108": [
    "clean",
    0.3033682294037323
  ],
  "high_school_government_and_politics 117": [
    "clean",
    0.5443419197744457
  ],
  "high_school_government_and_politics 122": [
    "input-and-label contamination",
    0.9995
  ],
  "high_school_government_and_politics 149": [
    "clean",
    0.41573183760683763
  ],
  "high_school_government_and_politics 159": [
    "input-and-label contamination",
    0.8179012345679013
  ],
  "high_school_government_and_politics 163": [
    "clean",
    0.16355685131195336
  ],
  "high_school_government_and_politics 168": [
    "clean",
    0.36965460526315785
  ],
  "high_school_government_and_politics 179": [
    "input-and-label contamination",
    0.9580182734719597
  ],
  "high_school_government_and_politics 186": [
    "input-and-label contamination",
    0.9295208362896741
  ],
  "sociology 41": [
    "input-and-label contamination",
    0.9561624146604384
  ],
  "sociology 43": [
    "clean",
    0.30528846153846156
  ],
  "sociology 49": [
    "clean",
    0.23473684210526316
  ],
  "sociology 55": [
    "clean",
    0.234375
  ],
  "sociology 70": [
    "input-and-label contamination",
    0.9950801749271136
  ],
  "sociology 94": [
    "input-and-label contamination",
    0.9485714285714285
  ],
  "sociology 173": [
    "clean",
    0.605
  ],
  "sociology 178": [
    "input contamination",
    0.7078410311493017
  ],
  "sociology 187": [
    "clean",
    0.20444606413994168
  ],
  "sociology 194": [
    "input-and-label contamination",
    0.9494459833795014
  ],
  "conceptual_physics 19": [
    "input-and-label contamination",
    0.9883381924198251
  ],
  "conceptual_physics 22": [
    "input-and-label contamination",
    0.9997724169321802
  ],
  "conceptual_physics 70": [
    "input-and-label contamination",
    0.9938552571688667
  ],
  "conceptual_physics 72": [
    "input-and-label contamination",
    0.75
  ],
  "conceptual_physics 94": [
    "input-and-label contamination",
    0.9268808114961962
  ],
  "conceptual_physics 123": [
    "input-and-label contamination",
    0.9507738095238095
  ],
  "conceptual_physics 133": [
    "clean",
    0.6552169421487604
  ],
  "conceptual_physics 142": [
    "input-and-label contamination",
    0.9382929642445215
  ],
  "conceptual_physics 189": [
    "input-and-label contamination",
    0.8833333333333334
  ],
  "conceptual_physics 192": [
    "input-and-label contamination",
    0.75
  ],
  "conceptual_physics 202": [
    "clean",
    0.6371428571428571
  ],
  "conceptual_physics 211": [
    "clean",
    0.46464646464646464
  ],
  "conceptual_physics 224": [
    "input-and-label contamination",
    0.9998177842565598
  ],
  "human_aging 7": [
    "clean",
    0.11624999999999999
  ],
  "human_aging 16": [
    "clean",
    0.21634615384615383
  ],
  "human_aging 52": [
    "clean",
    0.29733333333333334
  ],
  "human_aging 62": [
    "clean",
    0.15029761904761907
  ],
  "human_aging 84": [
    "clean",
    0.42592592592592593
  ],
  "human_aging 91": [
    "clean",
    0.1957894736842105
  ],
  "human_aging 96": [
    "clean",
    0.2847058823529412
  ],
  "human_aging 98": [
    "clean",
    0.1814516129032258
  ],
  "human_aging 125": [
    "clean",
    0.31857142857142856
  ],
  "human_aging 128": [
    "clean",
    0.26601562500000003
  ],
  "human_aging 129": [
    "clean",
    0.1753472222222222
  ],
  "human_aging 130": [
    "clean",
    0.22441520467836257
  ],
  "human_aging 134": [
    "clean",
    0.6724489795918367
  ],
  "human_aging 150": [
    "clean",
    0.3422373081463991
  ],
  "human_aging 156": [
    "clean",
    0.04761904761904762
  ],
  "human_aging 185": [
    "clean",
    0.18566176470588236
  ],
  "human_aging 214": [
    "clean",
    0.23979591836734693
  ],
  "high_school_psychology 1": [
    "clean",
    0.5
  ],
  "high_school_psychology 19": [
    "input-and-label contamination",
    0.9999375
  ],
  "high_school_psychology 27": [
    "input-and-label contamination",
    0.9597222222222221
  ],
  "high_school_psychology 33": [
    "input contamination",
    0.8421101974948129
  ],
  "high_school_psychology 61": [
    "clean",
    0.3457142857142857
  ],
  "high_school_psychology 67": [
    "input-and-label contamination",
    0.9999460101500918
  ],
  "high_school_psychology 86": [
    "clean",
    0.2842592592592593
  ],
  "high_school_psychology 93": [
    "input-and-label contamination",
    0.8660026174203869
  ],
  "high_school_psychology 145": [
    "input-and-label contamination",
    0.9999921875
  ],
  "high_school_psychology 164": [
    "input contamination",
    0.8315723789083065
  ],
  "high_school_psychology 170": [
    "input contamination",
    0.8749527588813303
  ],
  "high_school_psychology 178": [
    "input-and-label contamination",
    0.9564768235716853
  ],
  "high_school_psychology 199": [
    "input-and-label contamination",
    0.9561624146604384
  ],
  "high_school_psychology 204": [
    "input-and-label contamination",
    0.9997724169321802
  ],
  "high_school_psychology 223": [
    "clean",
    0.16245098039215686
  ],
  "high_school_psychology 225": [
    "input-and-label contamination",
    0.9921875
  ],
  "high_school_psychology 237": [
    "input-and-label contamination",
    0.9696838556851312
  ],
  "high_school_psychology 246": [
    "input-and-label contamination",
    0.999958905235473
  ],
  "high_school_psychology 249": [
    "input contamination",
    0.8233469472160471
  ],
  "high_school_psychology 262": [
    "clean",
    0.2024328249818446
  ],
  "high_school_psychology 267": [
    "clean",
    0.3430769230769231
  ],
  "high_school_psychology 281": [
    "clean",
    0.6668440984394601
  ],
  "high_school_psychology 319": [
    "input contamination",
    0.8359804866980545
  ],
  "high_school_psychology 352": [
    "input-and-label contamination",
    0.9997106481481481
  ],
  "high_school_psychology 365": [
    "input-and-label contamination",
    0.959722222222222
  ],
  "high_school_psychology 372": [
    "input-and-label contamination",
    0.9999375
  ],
  "high_school_psychology 381": [
    "input-and-label contamination",
    0.9997106481481481
  ],
  "high_school_psychology 418": [
    "input-and-label contamination",
    0.8848701131687242
  ],
  "high_school_psychology 423": [
    "input-and-label contamination",
    0.9413434903047091
  ],
  "high_school_psychology 436": [
    "clean",
    0.32266666666666666
  ],
  "high_school_psychology 489": [
    "input contamination",
    0.7192627317281056
  ],
  "high_school_psychology 495": [
    "input-and-label contamination",
    0.9999715521165226
  ],
  "high_school_psychology 505": [
    "input-and-label contamination",
    0.9998177842565598
  ],
  "high_school_psychology 522": [
    "input-and-label contamination",
    0.8267780172413793
  ],
  "high_school_psychology 530": [
    "clean",
    0.3716666666666667
  ],
  "high_school_psychology 538": [
    "input-and-label contamination",
    0.996
  ],
  "high_school_psychology 540": [
    "input-and-label contamination",
    0.9999892832647462
  ],
  "jurisprudence 19": [
    "input-and-label contamination",
    0.9375
  ],
  "jurisprudence 42": [
    "input-and-label contamination",
    0.9997724169321802
  ],
  "jurisprudence 45": [
    "input-and-label contamination",
    0.8313148788927335
  ],
  "jurisprudence 68": [
    "clean",
    0.14174107142857142
  ],
  "jurisprudence 71": [
    "input-and-label contamination",
    0.7271145144448312
  ],
  "jurisprudence 81": [
    "input-and-label contamination",
    0.9595386806925267
  ],
  "jurisprudence 83": [
    "clean",
    0.48130657221566314
  ],
  "jurisprudence 102": [
    "clean",
    0.33780020884093276
  ],
  "jurisprudence 103": [
    "clean",
    0.1753472222222222
  ],
  "moral_scenarios 6": [
    "clean",
    0.021739130434782608
  ],
  "moral_scenarios 20": [
    "clean",
    0.0136986301369863
  ],
  "moral_scenarios 34": [
    "clean",
    0.028985507246376812
  ],
  "moral_scenarios 51": [
    "clean",
    0.03125
  ],
  "moral_scenarios 55": [
    "clean",
    0
  ],
  "moral_scenarios 61": [
    "clean",
    0.021739130434782608
  ],
  "moral_scenarios 74": [
    "clean",
    0.022727272727272724
  ],
  "moral_scenarios 75": [
    "clean",
    0.017241379310344827
  ],
  "moral_scenarios 93": [
    "clean",
    0.008333333333333333
  ],
  "moral_scenarios 105": [
    "clean",
    0.0136986301369863
  ],
  "moral_scenarios 116": [
    "clean",
    0.020833333333333332
  ],
  "moral_scenarios 138": [
    "clean",
    0
  ],
  "moral_scenarios 140": [
    "clean",
    0.016666666666666666
  ],
  "moral_scenarios 144": [
    "clean",
    0.024193548387096774
  ],
  "moral_scenarios 173": [
    "clean",
    0.03278688524590164
  ],
  "moral_scenarios 184": [
    "clean",
    0.03125
  ],
  "moral_scenarios 216": [
    "clean",
    0.031746031746031744
  ],
  "moral_scenarios 219": [
    "clean",
    0.016129032258064516
  ],
  "moral_scenarios 231": [
    "clean",
    0.007352941176470588
  ],
  "moral_scenarios 239": [
    "clean",
    0.0496
  ],
  "moral_scenarios 245": [
    "clean",
    0.07493622448979592
  ],
  "moral_scenarios 263": [
    "clean",
    0.032467532467532464
  ],
  "moral_scenarios 279": [
    "clean",
    0.027777777777777776
  ],
  "moral_scenarios 288": [
    "clean",
    0.03508771929824561
  ],
  "moral_scenarios 300": [
    "clean",
    0.025423728813559324
  ],
  "moral_scenarios 313": [
    "clean",
    0.07351532567049808
  ],
  "moral_scenarios 341": [
    "clean",
    0.03205128205128205
  ],
  "moral_scenarios 354": [
    "clean",
    0.015873015873015872
  ],
  "moral_scenarios 356": [
    "clean",
    0.014084507042253521
  ],
  "moral_scenarios 394": [
    "clean",
    0.01639344262295082
  ],
  "moral_scenarios 398": [
    "clean",
    0.03225806451612903
  ],
  "moral_scenarios 407": [
    "clean",
    0.040983606557377046
  ],
  "moral_scenarios 420": [
    "clean",
    0.02857142857142857
  ],
  "moral_scenarios 537": [
    "clean",
    0.015384615384615385
  ],
  "moral_scenarios 558": [
    "clean",
    0.028985507246376812
  ],
  "moral_scenarios 562": [
    "clean",
    0.05314285714285714
  ],
  "moral_scenarios 565": [
    "clean",
    0.03278688524590164
  ],
  "moral_scenarios 577": [
    "clean",
    0.028985507246376812
  ],
  "moral_scenarios 614": [
    "clean",
    0.04794520547945205
  ],
  "moral_scenarios 634": [
    "clean",
    0.017241379310344827
  ],
  "moral_scenarios 651": [
    "clean",
    0.007936507936507936
  ],
  "moral_scenarios 668": [
    "clean",
    0.0078125
  ],
  "moral_scenarios 686": [
    "clean",
    0.0
  ],
  "moral_scenarios 690": [
    "clean",
    0.05537280701754386
  ],
  "moral_scenarios 694": [
    "clean",
    0.023076923076923078
  ],
  "moral_scenarios 721": [
    "clean",
    0.02112676056338028
  ],
  "moral_scenarios 741": [
    "clean",
    0.015625
  ],
  "moral_scenarios 747": [
    "clean",
    0.03389830508474576
  ],
  "moral_scenarios 750": [
    "clean",
    0.03676470588235294
  ],
  "moral_scenarios 752": [
    "clean",
    0.03333333333333333
  ],
  "moral_scenarios 756": [
    "clean",
    0.023809523809523808
  ],
  "moral_scenarios 767": [
    "clean",
    0.028846153846153848
  ],
  "moral_scenarios 769": [
    "clean",
    0.05723076923076923
  ],
  "moral_scenarios 771": [
    "clean",
    0.045454545454545456
  ],
  "moral_scenarios 776": [
    "clean",
    0.0234375
  ],
  "moral_scenarios 778": [
    "clean",
    0.024193548387096774
  ],
  "moral_scenarios 786": [
    "clean",
    0.03508771929824561
  ],
  "moral_scenarios 787": [
    "clean",
    0.03125
  ],
  "moral_scenarios 807": [
    "clean",
    0.03389830508474576
  ],
  "moral_scenarios 822": [
    "clean",
    0.015625
  ],
  "moral_scenarios 833": [
    "clean",
    0.12480630165289254
  ],
  "moral_scenarios 868": [
    "clean",
    0
  ],
  "moral_scenarios 890": [
    "clean",
    0.02857142857142857
  ],
  "college_medicine 11": [
    "input-and-label contamination",
    0.9981318422239535
  ],
  "college_medicine 30": [
    "input contamination",
    0.9444408845600695
  ],
  "college_medicine 72": [
    "clean",
    0.6114130434782609
  ],
  "college_medicine 75": [
    "clean",
    0.2869318181818182
  ],
  "college_medicine 80": [
    "clean",
    0.4849897119341564
  ],
  "college_medicine 85": [
    "input-and-label contamination",
    0.7433333333333334
  ],
  "college_medicine 101": [
    "clean",
    0.125
  ],
  "college_medicine 115": [
    "clean",
    0.1690909090909091
  ],
  "college_medicine 141": [
    "clean",
    0.5061379916384843
  ],
  "college_medicine 145": [
    "clean",
    0.23473684210526316
  ],
  "college_medicine 147": [
    "input-and-label contamination",
    0.8736225895316805
  ],
  "high_school_world_history 230": [
    "clean",
    0
  ],
  "high_school_world_history 235": [
    "clean",
    0.6580236372623915
  ],
  "virology 19": [
    "clean",
    0.21999999999999997
  ],
  "virology 22": [
    "clean",
    0.3716666666666667
  ],
  "virology 30": [
    "clean",
    0.21041666666666667
  ],
  "virology 48": [
    "input-and-label contamination",
    0.9945130315500685
  ],
  "virology 62": [
    "input contamination",
    0.9039276159334126
  ],
  "virology 73": [
    "clean",
    0.23249999999999998
  ],
  "virology 83": [
    "input-and-label contamination",
    0.9883381924198251
  ],
  "virology 95": [
    "input-and-label contamination",
    0.9905185185185185
  ],
  "virology 111": [
    "input-and-label contamination",
    0.9971528790087464
  ],
  "virology 115": [
    "input-and-label contamination",
    0.9021555367709215
  ],
  "high_school_statistics 3": [
    "input-and-label contamination",
    0.9163223140495868
  ],
  "high_school_statistics 21": [
    "clean",
    0.41565743944636674
  ],
  "high_school_statistics 48": [
    "clean",
    0.5100494544831987
  ],
  "high_school_statistics 82": [
    "input-and-label contamination",
    0.9963241598079561
  ],
  "high_school_statistics 108": [
    "clean",
    0.39453125
  ],
  "high_school_statistics 160": [
    "input contamination",
    0.8458359821996184
  ],
  "high_school_statistics 162": [
    "input contamination",
    0.9515943694340707
  ],
  "high_school_statistics 165": [
    "clean",
    0.3838720567136467
  ],
  "high_school_statistics 186": [
    "input-and-label contamination",
    0.9999915709974881
  ],
  "high_school_statistics 199": [
    "clean",
    0.5365839781520255
  ],
  "high_school_statistics 203": [
    "clean",
    0.1221917808219178
  ],
  "high_school_statistics 213": [
    "input-and-label contamination",
    0.9999962307106618
  ],
  "nutrition 4": [
    "input-and-label contamination",
    0.7361111111111112
  ],
  "nutrition 39": [
    "clean",
    0.3506944444444445
  ],
  "nutrition 44": [
    "clean",
    0.20089285714285712
  ],
  "nutrition 69": [
    "input-and-label contamination",
    0.9948631544341251
  ],
  "nutrition 84": [
    "input-and-label contamination",
    0.9921875
  ],
  "nutrition 94": [
    "clean",
    0.23379629629629628
  ],
  "nutrition 99": [
    "input-and-label contamination",
    0.999996
  ],
  "nutrition 118": [
    "clean",
    0.41425
  ],
  "nutrition 149": [
    "clean",
    0.3735827664399093
  ],
  "nutrition 161": [
    "input-and-label contamination",
    0.9530428249436514
  ],
  "nutrition 177": [
    "input-and-label contamination",
    0.9983125
  ],
  "nutrition 181": [
    "input-and-label contamination",
    0.9990234375
  ],
  "nutrition 204": [
    "input-and-label contamination",
    0.9950801749271136
  ],
  "nutrition 235": [
    "clean",
    0.5357142857142857
  ],
  "nutrition 249": [
    "input-and-label contamination",
    0.9950801749271136
  ],
  "nutrition 254": [
    "input-and-label contamination",
    0.9434137291280149
  ],
  "nutrition 263": [
    "input-and-label contamination",
    0.744140625
  ],
  "nutrition 270": [
    "clean",
    0.11272727272727273
  ],
  "nutrition 277": [
    "input-and-label contamination",
    0.9996243425995492
  ],
  "nutrition 304": [
    "input-and-label contamination",
    0.9976851851851852
  ],
  "abstract_algebra 25": [
    "input contamination",
    0.8518168452005882
  ],
  "abstract_algebra 32": [
    "clean",
    0.0375
  ],
  "abstract_algebra 47": [
    "clean",
    0.40760869565217384
  ],
  "high_school_geography 18": [
    "input-and-label contamination",
    0.9985422740524781
  ],
  "high_school_geography 25": [
    "clean",
    0.4955555555555556
  ],
  "high_school_geography 46": [
    "input-and-label contamination",
    0.9998779296875
  ],
  "high_school_geography 54": [
    "input-and-label contamination",
    0.9976851851851852
  ],
  "high_school_geography 62": [
    "clean",
    0.3308823529411765
  ],
  "high_school_geography 77": [
    "clean",
    0.30456349206349204
  ],
  "high_school_geography 80": [
    "clean",
    0.39453125
  ],
  "high_school_geography 83": [
    "clean",
    0.5294396961063628
  ],
  "high_school_geography 149": [
    "input-and-label contamination",
    0.9375
  ],
  "high_school_geography 190": [
    "input-and-label contamination",
    0.9997106481481481
  ],
  "econometrics 3": [
    "clean",
    0.3947923997185081
  ],
  "econometrics 11": [
    "clean",
    0.321125925925926
  ],
  "econometrics 21": [
    "input contamination",
    0.8573673469387755
  ],
  "econometrics 26": [
    "input-and-label contamination",
    0.9007523148148148
  ],
  "econometrics 40": [
    "input contamination",
    0.8510174668978744
  ],
  "econometrics 49": [
    "clean",
    0.5649840136292562
  ],
  "econometrics 57": [
    "clean",
    0.3271604938271605
  ],
  "econometrics 58": [
    "input-and-label contamination",
    0.9854346836595357
  ],
  "econometrics 62": [
    "clean",
    0.3193834688346883
  ],
  "econometrics 72": [
    "input-and-label contamination",
    0.997369935070272
  ],
  "marketing 2": [
    "clean",
    0.5563616071428571
  ],
  "marketing 18": [
    "clean",
    0.24183238636363638
  ],
  "marketing 25": [
    "input-and-label contamination",
    0.943675509419454
  ],
  "marketing 43": [
    "clean",
    0.13813813813813813
  ],
  "marketing 61": [
    "input-and-label contamination",
    0.9203250547845143
  ],
  "marketing 67": [
    "clean",
    0.17766203703703703
  ],
  "marketing 89": [
    "input-and-label contamination",
    0.9905185185185185
  ],
  "marketing 97": [
    "clean",
    0.30758620689655175
  ],
  "marketing 99": [
    "clean",
    0.2289795918367347
  ],
  "marketing 102": [
    "input-and-label contamination",
    0.9934866680236109
  ],
  "marketing 107": [
    "input-and-label contamination",
    0.8565549676660787
  ],
  "marketing 114": [
    "input contamination",
    0.9318114217727542
  ],
  "marketing 123": [
    "clean",
    0.5991253644314868
  ],
  "marketing 132": [
    "input-and-label contamination",
    0.943675509419454
  ],
  "marketing 134": [
    "clean",
    0.21319444444444444
  ],
  "marketing 150": [
    "input-and-label contamination",
    0.7894736842105263
  ],
  "marketing 164": [
    "clean",
    0.16611842105263158
  ],
  "marketing 172": [
    "input-and-label contamination",
    0.7052154195011338
  ],
  "marketing 176": [
    "clean",
    0.21296296296296297
  ],
  "marketing 183": [
    "input-and-label contamination",
    0.9995680812007343
  ],
  "marketing 186": [
    "clean",
    0.336734693877551
  ],
  "high_school_chemistry 17": [
    "input contamination",
    0.9228098290598291
  ],
  "high_school_chemistry 37": [
    "input-and-label contamination",
    0.9219047619047618
  ],
  "high_school_chemistry 51": [
    "clean",
    0.2759538598047915
  ],
  "high_school_chemistry 65": [
    "clean",
    0.3793845663265306
  ],
  "high_school_chemistry 81": [
    "clean",
    0.40889212827988336
  ],
  "high_school_chemistry 83": [
    "input contamination",
    0.8950617283950617
  ],
  "high_school_chemistry 100": [
    "input-and-label contamination",
    0.8790711814261358
  ],
  "high_school_chemistry 127": [
    "clean",
    0.4077124183006535
  ],
  "high_school_chemistry 144": [
    "input contamination",
    0.7111822354681517
  ],
  "high_school_chemistry 148": [
    "clean",
    0.673828125
  ],
  "high_school_chemistry 198": [
    "clean",
    0.40178571428571425
  ],
  "prehistory 1": [
    "clean",
    0.4503761574074074
  ],
  "prehistory 6": [
    "clean",
    0.17857142857142858
  ],
  "prehistory 18": [
    "clean",
    0.21319444444444444
  ],
  "prehistory 29": [
    "input-and-label contamination",
    0.9597222222222221
  ],
  "prehistory 33": [
    "input-and-label contamination",
    0.9241481481481482
  ],
  "prehistory 72": [
    "input-and-label contamination",
    0.9999872786484836
  ],
  "prehistory 84": [
    "clean",
    0.3735827664399093
  ],
  "prehistory 126": [
    "clean",
    0.09868421052631579
  ],
  "prehistory 169": [
    "input-and-label contamination",
    0.996
  ],
  "prehistory 170": [
    "input-and-label contamination",
    0.811875924556213
  ],
  "prehistory 182": [
    "input-and-label contamination",
    0.9536471433704636
  ],
  "prehistory 203": [
    "input-and-label contamination",
    0.8588429752066116
  ],
  "prehistory 219": [
    "clean",
    0.22549019607843138
  ],
  "prehistory 227": [
    "input-and-label contamination",
    0.9908878845312727
  ],
  "prehistory 278": [
    "input-and-label contamination",
    0.9999530428249437
  ],
  "prehistory 322": [
    "clean",
    0.19444444444444445
  ],
  "college_physics 1": [
    "input-and-label contamination",
    0.8471470220741357
  ],
  "college_physics 3": [
    "clean",
    0.242
  ],
  "college_physics 29": [
    "input-and-label contamination",
    0.954564351770586
  ],
  "college_physics 34": [
    "input-and-label contamination",
    0.9112861570247933
  ],
  "college_physics 44": [
    "input-and-label contamination",
    0.9554178339097819
  ],
  "college_physics 56": [
    "input contamination",
    0.9156614207475657
  ],
  "college_physics 93": [
    "input-and-label contamination",
    0.94536377084454
  ],
  "college_physics 98": [
    "input contamination",
    0.9443483275663207
  ],
  "management 10": [
    "input-and-label contamination",
    0.9997724169321802
  ],
  "management 25": [
    "input-and-label contamination",
    0.9995
  ],
  "management 26": [
    "clean",
    0.5111111111111111
  ],
  "management 54": [
    "clean",
    0.1703703703703704
  ],
  "management 55": [
    "input-and-label contamination",
    0.999136
  ],
  "management 71": [
    "clean",
    0.6453333333333333
  ],
  "management 87": [
    "input contamination",
    0.8753255208333333
  ],
  "management 97": [
    "input-and-label contamination",
    0.9988148148148148
  ],
  "college_biology 5": [
    "clean",
    0.06666666666666667
  ],
  "college_biology 38": [
    "clean",
    0.23979591836734693
  ],
  "college_biology 68": [
    "clean",
    0.4326923076923077
  ],
  "college_biology 77": [
    "clean",
    0.3506944444444444
  ],
  "college_biology 84": [
    "clean",
    0.4955555555555556
  ],
  "college_biology 127": [
    "clean",
    0.35404829545454547
  ],
  "college_biology 129": [
    "clean",
    0.21799628942486085
  ],
  "college_biology 135": [
    "clean",
    0.009615384615384616
  ],
  "high_school_biology 3": [
    "input-and-label contamination",
    0.983616
  ],
  "high_school_biology 9": [
    "input contamination",
    0.7441406249999999
  ],
  "high_school_biology 14": [
    "clean",
    0.2623529411764706
  ],
  "high_school_biology 24": [
    "clean",
    0.599158653846154
  ],
  "high_school_biology 29": [
    "input-and-label contamination",
    0.9999375
  ],
  "high_school_biology 31": [
    "clean",
    0.40545454545454546
  ],
  "high_school_biology 47": [
    "input-and-label contamination",
    0.9999937112455507
  ],
  "high_school_biology 52": [
    "clean",
    0.5724489795918367
  ],
  "high_school_biology 70": [
    "input-and-label contamination",
    0.9999745973682873
  ],
  "high_school_biology 84": [
    "input-and-label contamination",
    0.9996243425995492
  ],
  "high_school_biology 112": [
    "input contamination",
    0.7263247759009696
  ],
  "high_school_biology 122": [
    "clean",
    0.3190335305719921
  ],
  "high_school_biology 126": [
    "clean",
    0.2630208333333333
  ],
  "high_school_biology 144": [
    "input contamination",
    0.7161032990805841
  ],
  "high_school_biology 155": [
    "clean",
    0.5111111111111111
  ],
  "high_school_biology 162": [
    "input-and-label contamination",
    0.9086363636363637
  ],
  "high_school_biology 167": [
    "input-and-label contamination",
    0.9997724169321802
  ],
  "high_school_biology 169": [
    "input-and-label contamination",
    0.8950617283950617
  ],
  "high_school_biology 181": [
    "input-and-label contamination",
    0.9999794989544467
  ],
  "high_school_biology 184": [
    "input contamination",
    0.9016175087927012
  ],
  "high_school_biology 190": [
    "input-and-label contamination",
    0.9996565235090576
  ],
  "high_school_biology 227": [
    "input-and-label contamination",
    0.9921875
  ],
  "high_school_biology 229": [
    "clean",
    0.166259765625
  ],
  "high_school_biology 239": [
    "clean",
    0.29733333333333334
  ],
  "high_school_biology 249": [
    "clean",
    0.09345588235294117
  ],
  "high_school_biology 261": [
    "clean",
    0.28527154663518295
  ],
  "high_school_biology 264": [
    "clean",
    0.11012645107794361
  ],
  "high_school_physics 45": [
    "input contamination",
    0.8629264372854116
  ],
  "high_school_physics 60": [
    "input-and-label contamination",
    0.997511643766051
  ],
  "high_school_physics 77": [
    "input-and-label contamination",
    0.8838044847568657
  ],
  "high_school_physics 102": [
    "input-and-label contamination",
    0.9902423759672281
  ],
  "high_school_physics 138": [
    "clean",
    0.2420485850237916
  ],
  "logical_fallacies 39": [
    "clean",
    0.14880000000000002
  ],
  "logical_fallacies 63": [
    "clean",
    0.186
  ],
  "logical_fallacies 73": [
    "input contamination",
    0.7890293983443062
  ],
  "logical_fallacies 93": [
    "clean",
    0.5563616071428571
  ],
  "logical_fallacies 130": [
    "clean",
    0.10883620689655173
  ],
  "logical_fallacies 144": [
    "clean",
    0.10222222222222221
  ],
  "logical_fallacies 145": [
    "clean",
    0.09090909090909091
  ],
  "logical_fallacies 148": [
    "clean",
    0.12377450980392157
  ],
  "logical_fallacies 161": [
    "clean",
    0.21156462585034014
  ],
  "medical_genetics 39": [
    "clean",
    0.3966836734693877
  ],
  "medical_genetics 63": [
    "clean",
    0.06666666666666667
  ],
  "medical_genetics 65": [
    "input-and-label contamination",
    0.9224489795918368
  ],
  "medical_genetics 89": [
    "clean",
    0.4791666666666667
  ],
  "medical_genetics 95": [
    "clean",
    0.5353535353535354
  ],
  "medical_genetics 96": [
    "clean",
    0.5402644230769232
  ],
  "machine_learning 2": [
    "clean",
    0.3816496127536888
  ],
  "machine_learning 9": [
    "input-and-label contamination",
    0.9994168246100015
  ],
  "machine_learning 16": [
    "clean",
    0.6471717171717171
  ],
  "machine_learning 17": [
    "input-and-label contamination",
    0.9013255180031713
  ],
  "machine_learning 38": [
    "input-and-label contamination",
    0.9259259259259258
  ],
  "machine_learning 99": [
    "input-and-label contamination",
    0.7817752354018703
  ],
  "machine_learning 106": [
    "clean",
    0.09370760233918128
  ],
  "professional_law_1 1": [
    "clean",
    0.03644349477682811
  ],
  "professional_law_1 15": [
    "clean",
    0.46102347428105006
  ],
  "professional_law_1 16": [
    "clean",
    0.5075086638428956
  ],
  "professional_law_1 17": [
    "clean",
    0.05489130434782609
  ],
  "professional_law_1 184": [
    "clean",
    0.05633981403212172
  ],
  "professional_law_1 191": [
    "clean",
    0.34099484964440324
  ],
  "professional_law_1 203": [
    "clean",
    0.3558677685950413
  ],
  "professional_law_1 249": [
    "clean",
    0.2036290322580645
  ],
  "professional_law_1 252": [
    "clean",
    0.06090899158745766
  ],
  "professional_law_1 255": [
    "clean",
    0.04611545138888889
  ],
  "professional_law_1 281": [
    "clean",
    0.03605769230769231
  ],
  "professional_law_1 346": [
    "clean",
    0.09844529527787445
  ],
  "professional_law_1 361": [
    "clean",
    0.038275629496402876
  ],
  "professional_law_1 363": [
    "clean",
    0.07559322033898305
  ],
  "professional_law_1 373": [
    "input-and-label contamination",
    0.9112723214285714
  ],
  "professional_law_1 384": [
    "clean",
    0.5649298941798941
  ],
  "professional_law_1 401": [
    "clean",
    0.01692420897718911
  ],
  "professional_law_1 441": [
    "input contamination",
    0.7016765285996055
  ],
  "professional_law_1 443": [
    "clean",
    0.04319351317636052
  ],
  "professional_law_1 519": [
    "clean",
    0.1073713067316479
  ],
  "professional_law_1 530": [
    "clean",
    0.03618421052631579
  ],
  "professional_law_1 551": [
    "clean",
    0.06861413043478261
  ],
  "professional_law_1 574": [
    "clean",
    0.10529150616961545
  ],
  "professional_law_1 600": [
    "clean",
    0.10535272277227722
  ],
  "professional_law_1 604": [
    "clean",
    0.053473937345638885
  ],
  "professional_law_1 639": [
    "clean",
    0.09433042880073376
  ],
  "professional_law_1 645": [
    "clean",
    0.33900650157060414
  ],
  "professional_law_1 688": [
    "clean",
    0.31340657487948276
  ],
  "professional_law_1 713": [
    "clean",
    0.4427527883960831
  ],
  "professional_law_1 744": [
    "clean",
    0
  ],
  "professional_law_1 765": [
    "clean",
    0.40483316317316165
  ],
  "professional_psychology 12": [
    "clean",
    0.3154761904761906
  ],
  "professional_psychology 22": [
    "clean",
    0.09540816326530611
  ],
  "professional_psychology 37": [
    "clean",
    0.09090909090909091
  ],
  "professional_psychology 54": [
    "input-and-label contamination",
    0.9999915709974881
  ],
  "professional_psychology 78": [
    "clean",
    0.1597222222222222
  ],
  "professional_psychology 91": [
    "clean",
    0.6520030737704917
  ],
  "professional_psychology 104": [
    "clean",
    0.15781250000000002
  ],
  "professional_psychology 111": [
    "input-and-label contamination",
    0.9494459833795015
  ],
  "professional_psychology 114": [
    "clean",
    0.20444444444444443
  ],
  "professional_psychology 125": [
    "clean",
    0.3
  ],
  "professional_psychology 129": [
    "clean",
    0.3173469387755102
  ],
  "professional_psychology 150": [
    "clean",
    0.551907719609583
  ],
  "professional_psychology 161": [
    "clean",
    0.15901360544217685
  ],
  "professional_psychology 162": [
    "input-and-label contamination",
    0.864795918367347
  ],
  "professional_psychology 211": [
    "clean",
    0.3716666666666667
  ],
  "professional_psychology 240": [
    "clean",
    0.1476032273374466
  ],
  "professional_psychology 246": [
    "clean",
    0.33622448979591835
  ],
  "professional_psychology 265": [
    "clean",
    0.43557475582268973
  ],
  "professional_psychology 282": [
    "input-and-label contamination",
    0.8660869565217391
  ],
  "professional_psychology 346": [
    "clean",
    0.13953488372093023
  ],
  "professional_psychology 350": [
    "clean",
    0.4240362811791383
  ],
  "professional_psychology 359": [
    "clean",
    0.40350877192982454
  ],
  "professional_psychology 360": [
    "clean",
    0.2847058823529412
  ],
  "professional_psychology 375": [
    "clean",
    0.3381818181818182
  ],
  "professional_psychology 377": [
    "clean",
    0.49416909620991256
  ],
  "professional_psychology 387": [
    "clean",
    0.3245442708333333
  ],
  "professional_psychology 411": [
    "clean",
    0.1950954861111111
  ],
  "professional_psychology 441": [
    "clean",
    0.3125
  ],
  "professional_psychology 450": [
    "input contamination",
    0.7707674619143432
  ],
  "professional_psychology 463": [
    "clean",
    0.36507936507936506
  ],
  "professional_psychology 509": [
    "clean",
    0.22544642857142855
  ],
  "professional_psychology 538": [
    "clean",
    0.3716666666666667
  ],
  "professional_psychology 551": [
    "clean",
    0.1598639455782313
  ],
  "professional_psychology 556": [
    "input-and-label contamination",
    0.7982731554160126
  ],
  "professional_psychology 571": [
    "clean",
    0.010638297872340425
  ],
  "professional_psychology 575": [
    "clean",
    0.260204081632653
  ],
  "professional_psychology 583": [
    "clean",
    0.155
  ],
  "professional_psychology 591": [
    "clean",
    0.1419753086419753
  ],
  "professional_psychology 608": [
    "clean",
    0.6146384479717814
  ],
  "global_facts 15": [
    "clean",
    0.5694117647058824
  ],
  "global_facts 17": [
    "clean",
    0.5991586538461539
  ],
  "global_facts 18": [
    "clean",
    0.3577563070316694
  ],
  "global_facts 38": [
    "clean",
    0.2560386473429952
  ],
  "us_foreign_policy 45": [
    "clean",
    0.2630208333333333
  ],
  "us_foreign_policy 58": [
    "clean",
    0.1111111111111111
  ],
  "us_foreign_policy 63": [
    "clean",
    0.19391304347826085
  ],
  "us_foreign_policy 66": [
    "clean",
    0.21041666666666667
  ],
  "us_foreign_policy 68": [
    "input-and-label contamination",
    0.9950801749271136
  ],
  "us_foreign_policy 87": [
    "input contamination",
    0.8568948412698412
  ],
  "us_foreign_policy 93": [
    "clean",
    0.12
  ],
  "us_foreign_policy 96": [
    "clean",
    0.234375
  ],
  "professional_law_0 33": [
    "clean",
    0.3618471258452091
  ],
  "professional_law_0 90": [
    "clean",
    0.04526748971193416
  ],
  "professional_law_0 139": [
    "clean",
    0.4749742452745677
  ],
  "professional_law_0 140": [
    "clean",
    0.3506944444444444
  ],
  "professional_law_0 142": [
    "clean",
    0.585503054257995
  ],
  "professional_law_0 146": [
    "clean",
    0.06343283582089553
  ],
  "professional_law_0 160": [
    "clean",
    0.5865118318505212
  ],
  "professional_law_0 197": [
    "clean",
    0.030674846625766874
  ],
  "professional_law_0 431": [
    "clean",
    0.04891304347826087
  ],
  "professional_law_0 464": [
    "clean",
    0.06909496753246755
  ],
  "professional_law_0 501": [
    "clean",
    0.4294372574575817
  ],
  "professional_law_0 510": [
    "clean",
    0.08475858671171171
  ],
  "professional_law_0 543": [
    "clean",
    0.28868198449196064
  ],
  "professional_law_0 552": [
    "clean",
    0.09309382334905161
  ],
  "professional_law_0 613": [
    "clean",
    0.07041035887189734
  ],
  "professional_law_0 620": [
    "clean",
    0.057179487179487176
  ],
  "professional_law_0 638": [
    "clean",
    0.37276388072786965
  ],
  "professional_law_0 662": [
    "clean",
    0.4192949907235622
  ],
  "professional_law_0 729": [
    "clean",
    0.5069972826086957
  ],
  "international_law 17": [
    "input-and-label contamination",
    0.9999772230320699
  ],
  "international_law 31": [
    "clean",
    0.3916227781435154
  ],
  "international_law 37": [
    "clean",
    0.46464646464646464
  ],
  "international_law 44": [
    "clean",
    0.6354359925788496
  ],
  "international_law 97": [
    "clean",
    0.5925202546296297
  ],
  "international_law 100": [
    "input-and-label contamination",
    0.7569098658114608
  ],
  "international_law 101": [
    "clean",
    0.21799628942486085
  ],
  "international_law 120": [
    "clean",
    0.6565798733286418
  ],
  "clinical_knowledge 7": [
    "input-and-label contamination",
    0.9993850218658892
  ],
  "clinical_knowledge 17": [
    "input-and-label contamination",
    0.9995680812007343
  ],
  "clinical_knowledge 18": [
    "input-and-label contamination",
    0.8982291878689191
  ],
  "clinical_knowledge 23": [
    "clean",
    0.21201814058956914
  ],
  "clinical_knowledge 31": [
    "clean",
    0.16358024691358025
  ],
  "clinical_knowledge 64": [
    "input-and-label contamination",
    0.9759579263711495
  ],
  "clinical_knowledge 93": [
    "input-and-label contamination",
    0.9467186484730344
  ],
  "clinical_knowledge 98": [
    "clean",
    0.2647058823529412
  ],
  "clinical_knowledge 100": [
    "clean",
    0.31
  ],
  "clinical_knowledge 103": [
    "input-and-label contamination",
    0.8880208333333333
  ],
  "clinical_knowledge 117": [
    "clean",
    0.045454545454545456
  ],
  "clinical_knowledge 130": [
    "input-and-label contamination",
    0.7789062500000001
  ],
  "clinical_knowledge 145": [
    "input-and-label contamination",
    0.8375715193897013
  ],
  "clinical_knowledge 204": [
    "clean",
    0.4411764705882352
  ],
  "clinical_knowledge 207": [
    "clean",
    0.2676767676767677
  ],
  "clinical_knowledge 236": [
    "clean",
    0.5353535353535354
  ],
  "clinical_knowledge 242": [
    "clean",
    0.6312500000000001
  ],
  "high_school_mathematics 2": [
    "input-and-label contamination",
    0.8152220507544582
  ],
  "high_school_mathematics 10": [
    "clean",
    0.23249999999999998
  ],
  "high_school_mathematics 21": [
    "clean",
    0.29583715947352307
  ],
  "high_school_mathematics 27": [
    "input-and-label contamination",
    0.9833141817485397
  ],
  "high_school_mathematics 40": [
    "input contamination",
    0.8864888888888889
  ],
  "high_school_mathematics 54": [
    "clean",
    0.09491131756756757
  ],
  "high_school_mathematics 75": [
    "clean",
    0.3506944444444444
  ],
  "high_school_mathematics 102": [
    "input-and-label contamination",
    0.9854346836595357
  ],
  "high_school_mathematics 127": [
    "clean",
    0.335385101010101
  ],
  "high_school_mathematics 133": [
    "clean",
    0.15724322515891603
  ],
  "high_school_mathematics 139": [
    "clean",
    0.28425925925925927
  ],
  "high_school_mathematics 168": [
    "clean",
    0.5761904761904761
  ],
  "high_school_mathematics 215": [
    "clean",
    0.576923076923077
  ],
  "high_school_mathematics 241": [
    "clean",
    0.2368827160493827
  ],
  "high_school_computer_science 4": [
    "input-and-label contamination",
    0.9999945130315501
  ],
  "high_school_computer_science 6": [
    "clean",
    0.5744008189810912
  ],
  "high_school_computer_science 30": [
    "input contamination",
    0.8568948412698412
  ],
  "high_school_computer_science 35": [
    "clean",
    0.40889212827988336
  ],
  "high_school_computer_science 66": [
    "clean",
    0.35167162296116067
  ],
  "high_school_computer_science 69": [
    "clean",
    0.07985777436094561
  ],
  "college_computer_science 11": [
    "clean",
    0.17073170731707318
  ],
  "college_computer_science 19": [
    "input contamination",
    0.8147238849399344
  ],
  "college_computer_science 40": [
    "clean",
    0.19166666666666668
  ],
  "college_computer_science 71": [
    "clean",
    0.6850708037310909
  ],
  "college_computer_science 72": [
    "clean",
    0.13043478260869565
  ],
  "college_computer_science 76": [
    "clean",
    0.47959183673469397
  ],
  "college_computer_science 79": [
    "input contamination",
    0.8908068607777062
  ],
  "college_computer_science 84": [
    "clean",
    0.27486249999999995
  ],
  "electrical_engineering 17": [
    "clean",
    0.5769944341372912
  ],
  "electrical_engineering 60": [
    "input-and-label contamination",
    0.7937500000000001
  ],
  "electrical_engineering 64": [
    "clean",
    0.446
  ],
  "electrical_engineering 65": [
    "input-and-label contamination",
    0.9736328125
  ],
  "electrical_engineering 72": [
    "clean",
    0.42592592592592593
  ],
  "electrical_engineering 81": [
    "clean",
    0.28622448979591836
  ],
  "electrical_engineering 112": [
    "input-and-label contamination",
    0.9999460101500918
  ],
  "electrical_engineering 123": [
    "clean",
    0.29605263157894735
  ],
  "electrical_engineering 130": [
    "input-and-label contamination",
    0.9736328125
  ],
  "electrical_engineering 139": [
    "clean",
    0.21296296296296297
  ],
  "electrical_engineering 141": [
    "clean",
    0.41500000000000004
  ],
  "college_mathematics 1": [
    "input-and-label contamination",
    0.8661887694145758
  ],
  "college_mathematics 19": [
    "clean",
    0.06060606060606061
  ],
  "college_mathematics 24": [
    "clean",
    0.03125
  ],
  "college_mathematics 31": [
    "input contamination",
    0.8300000000000001
  ],
  "college_mathematics 71": [
    "clean",
    0.36743079584775085
  ],
  "computer_security 21": [
    "clean",
    0.13641826923076922
  ],
  "computer_security 23": [
    "clean",
    0.48822605965463106
  ],
  "computer_security 24": [
    "clean",
    0.23232323232323232
  ],
  "computer_security 35": [
    "clean",
    0.3173469387755102
  ],
  "computer_security 46": [
    "clean",
    0.4097589653145209
  ],
  "computer_security 77": [
    "clean",
    0.3265086206896552
  ],
  "computer_security 82": [
    "input-and-label contamination",
    0.9999901289163524
  ],
  "computer_security 87": [
    "input-and-label contamination",
    0.7155612244897958
  ],
  "high_school_macroeconomics 20": [
    "clean",
    0.20833333333333331
  ],
  "high_school_macroeconomics 24": [
    "input-and-label contamination",
    0.998179335457442
  ],
  "high_school_macroeconomics 43": [
    "input-and-label contamination",
    0.9952277596905696
  ],
  "high_school_macroeconomics 48": [
    "input-and-label contamination",
    0.7598601767576837
  ],
  "high_school_macroeconomics 58": [
    "input-and-label contamination",
    0.8228395061728395
  ],
  "high_school_macroeconomics 59": [
    "input-and-label contamination",
    0.9993141289437586
  ],
  "high_school_macroeconomics 73": [
    "input-and-label contamination",
    0.920138888888889
  ],
  "high_school_macroeconomics 80": [
    "clean",
    0.2350718065003779
  ],
  "high_school_macroeconomics 96": [
    "input-and-label contamination",
    0.8799999999999999
  ],
  "high_school_macroeconomics 129": [
    "clean",
    0.1237281976744186
  ],
  "high_school_macroeconomics 148": [
    "clean",
    0.2869318181818182
  ],
  "high_school_macroeconomics 149": [
    "input-and-label contamination",
    0.9494459833795015
  ],
  "high_school_macroeconomics 151": [
    "clean",
    0.351171875
  ],
  "high_school_macroeconomics 155": [
    "clean",
    0.30666666666666664
  ],
  "high_school_macroeconomics 161": [
    "input-and-label contamination",
    0.9021555367709213
  ],
  "high_school_macroeconomics 193": [
    "input-and-label contamination",
    0.976672
  ],
  "high_school_macroeconomics 227": [
    "input-and-label contamination",
    0.9972521880724609
  ],
  "high_school_macroeconomics 231": [
    "input-and-label contamination",
    0.8650887573964497
  ],
  "high_school_macroeconomics 235": [
    "clean",
    0.37758875739644965
  ],
  "high_school_macroeconomics 244": [
    "input-and-label contamination",
    0.9298245614035088
  ],
  "high_school_macroeconomics 257": [
    "input-and-label contamination",
    0.999744
  ],
  "high_school_macroeconomics 289": [
    "clean",
    0.5636849781234808
  ],
  "high_school_macroeconomics 298": [
    "input-and-label contamination",
    0.9558128544423441
  ],
  "high_school_macroeconomics 316": [
    "input-and-label contamination",
    0.9485714285714284
  ],
  "high_school_macroeconomics 317": [
    "clean",
    0.28125
  ],
  "high_school_macroeconomics 340": [
    "clean",
    0.24338624338624337
  ],
  "high_school_macroeconomics 363": [
    "input-and-label contamination",
    0.997369935070272
  ],
  "high_school_macroeconomics 370": [
    "clean",
    0.3005952380952381
  ],
  "high_school_macroeconomics 382": [
    "input-and-label contamination",
    0.9932512687614728
  ],
  "high_school_macroeconomics 389": [
    "clean",
    0.36079545454545453
  ],
  "astronomy 12": [
    "clean",
    0.5991586538461539
  ],
  "astronomy 48": [
    "input-and-label contamination",
    0.9073837566813302
  ],
  "astronomy 56": [
    "clean",
    0.6834075015893197
  ],
  "astronomy 66": [
    "input contamination",
    0.8404990748355262
  ],
  "astronomy 76": [
    "clean",
    0.24201161946259986
  ],
  "astronomy 121": [
    "clean",
    0.30721551176096623
  ],
  "astronomy 122": [
    "clean",
    0.40889212827988336
  ],
  "astronomy 142": [
    "input-and-label contamination",
    0.9999142661179699
  ],
  "college_chemistry 3": [
    "clean",
    0.08108108108108109
  ],
  "college_chemistry 5": [
    "input-and-label contamination",
    0.922865595942519
  ],
  "college_chemistry 41": [
    "input-and-label contamination",
    0.9402573529411763
  ],
  "college_chemistry 54": [
    "input-and-label contamination",
    0.7932387096774194
  ],
  "college_chemistry 64": [
    "clean",
    0.31
  ],
  "college_chemistry 90": [
    "input-and-label contamination",
    0.920940170940171
  ],
  "high_school_european_history 79": [
    "clean",
    0.38260843337153894
  ],
  "high_school_european_history 82": [
    "clean",
    0.15753636047991978
  ],
  "miscellaneous 18": [
    "clean",
    0.23232323232323232
  ],
  "miscellaneous 24": [
    "clean",
    0.5289115646258503
  ],
  "miscellaneous 48": [
    "clean",
    0.351171875
  ],
  "miscellaneous 55": [
    "input contamination",
    0.7111926741556371
  ],
  "miscellaneous 72": [
    "clean",
    0.23979591836734693
  ],
  "miscellaneous 92": [
    "clean",
    0.3823731138545953
  ],
  "miscellaneous 117": [
    "clean",
    0.2664930555555556
  ],
  "miscellaneous 122": [
    "clean",
    0.2839506172839506
  ],
  "miscellaneous 127": [
    "clean",
    0.4770408163265306
  ],
  "miscellaneous 129": [
    "clean",
    0.605
  ],
  "miscellaneous 171": [
    "clean",
    0.48557692307692313
  ],
  "miscellaneous 176": [
    "clean",
    0.6918367346938775
  ],
  "miscellaneous 180": [
    "clean",
    0.5623849355639158
  ],
  "miscellaneous 185": [
    "input-and-label contamination",
    0.7077444659700829
  ],
  "miscellaneous 188": [
    "clean",
    0.35778061224489793
  ],
  "miscellaneous 189": [
    "clean",
    0.5111111111111111
  ],
  "miscellaneous 196": [
    "clean",
    0.2839506172839506
  ],
  "miscellaneous 215": [
    "clean",
    0.2630208333333333
  ],
  "miscellaneous 223": [
    "clean",
    0.48822605965463106
  ],
  "miscellaneous 235": [
    "clean",
    0.21041666666666667
  ],
  "miscellaneous 258": [
    "clean",
    0.48509883880615234
  ],
  "miscellaneous 291": [
    "clean",
    0.43016975308641975
  ],
  "miscellaneous 341": [
    "clean",
    0.13450292397660818
  ],
  "miscellaneous 342": [
    "clean",
    0.6818181818181819
  ],
  "miscellaneous 354": [
    "clean",
    0.10054054054054054
  ],
  "miscellaneous 406": [
    "clean",
    0.6360544217687074
  ],
  "miscellaneous 410": [
    "clean",
    0.09375
  ],
  "miscellaneous 429": [
    "clean",
    0.21882352941176472
  ],
  "miscellaneous 473": [
    "input contamination",
    0.7971737874838044
  ],
  "miscellaneous 478": [
    "input-and-label contamination",
    0.9999772230320699
  ],
  "miscellaneous 480": [
    "clean",
    0.5111111111111111
  ],
  "miscellaneous 494": [
    "input-and-label contamination",
    0.9999860867629463
  ],
  "miscellaneous 524": [
    "clean",
    0.351171875
  ],
  "miscellaneous 525": [
    "clean",
    0.37875
  ],
  "miscellaneous 566": [
    "clean",
    0.4955555555555556
  ],
  "miscellaneous 634": [
    "clean",
    0.234375
  ],
  "miscellaneous 637": [
    "clean",
    0.15789473684210525
  ],
  "miscellaneous 645": [
    "clean",
    0.625
  ],
  "miscellaneous 648": [
    "clean",
    0.3381818181818182
  ],
  "miscellaneous 663": [
    "clean",
    0.654320987654321
  ],
  "miscellaneous 681": [
    "clean",
    0.6384943181818182
  ],
  "miscellaneous 692": [
    "clean",
    0.17857142857142855
  ],
  "miscellaneous 719": [
    "clean",
    0.21428571428571427
  ],
  "miscellaneous 748": [
    "clean",
    0.10000000000000002
  ],
  "miscellaneous 753": [
    "clean",
    0.4803206997084548
  ],
  "miscellaneous 776": [
    "input-and-label contamination",
    0.7822671156004491
  ],
  "formal_logic 1": [
    "input-and-label contamination",
    0.9999460101500918
  ],
  "formal_logic 3": [
    "input-and-label contamination",
    0.8196192742415228
  ],
  "formal_logic 4": [
    "clean",
    0.6598213132747934
  ],
  "formal_logic 12": [
    "input contamination",
    0.7632211538461539
  ],
  "formal_logic 32": [
    "clean",
    0.4452578815459702
  ],
  "formal_logic 42": [
    "clean",
    0.3830965909090909
  ],
  "formal_logic 68": [
    "clean",
    0.640096618357488
  ],
  "formal_logic 75": [
    "input contamination",
    0.9113515333292525
  ],
  "formal_logic 80": [
    "input contamination",
    0.765032261016794
  ],
  "formal_logic 82": [
    "clean",
    0.463517258755354
  ],
  "formal_logic 85": [
    "clean",
    0.49992592592592594
  ],
  "formal_logic 88": [
    "clean",
    0.16908824313516396
  ],
  "formal_logic 90": [
    "clean",
    0.6656948493683187
  ],
  "formal_logic 104": [
    "clean",
    0.685137169621947
  ],
  "formal_logic 105": [
    "input-and-label contamination",
    0.9298245614035088
  ],
  "formal_logic 109": [
    "clean",
    0.29145841343073176
  ],
  "elementary_mathematics 1": [
    "clean",
    0.6523749999999999
  ],
  "elementary_mathematics 12": [
    "input contamination",
    0.9722108843537415
  ],
  "elementary_mathematics 16": [
    "clean",
    0.16666666666666666
  ],
  "elementary_mathematics 47": [
    "clean",
    0.14285714285714285
  ],
  "elementary_mathematics 53": [
    "clean",
    0.305366847826087
  ],
  "elementary_mathematics 68": [
    "clean",
    0.36891679748822603
  ],
  "elementary_mathematics 74": [
    "input-and-label contamination",
    0.9999715521165226
  ],
  "elementary_mathematics 87": [
    "clean",
    0.41666666666666663
  ],
  "elementary_mathematics 91": [
    "input contamination",
    0.9615076923076923
  ],
  "elementary_mathematics 136": [
    "input-and-label contamination",
    0.9767375942625112
  ],
  "elementary_mathematics 150": [
    "input contamination",
    0.9741260632282807
  ],
  "elementary_mathematics 158": [
    "clean",
    0.6317148760330579
  ],
  "elementary_mathematics 174": [
    "clean",
    0.125
  ],
  "elementary_mathematics 187": [
    "clean",
    0.2763043478260869
  ],
  "elementary_mathematics 190": [
    "clean",
    0.2367142857142857
  ],
  "elementary_mathematics 192": [
    "clean",
    0.428369341563786
  ],
  "elementary_mathematics 200": [
    "input contamination",
    0.8504464285714285
  ],
  "elementary_mathematics 211": [
    "input-and-label contamination",
    0.9832164076398913
  ],
  "elementary_mathematics 256": [
    "clean",
    0.1815364387678437
  ],
  "elementary_mathematics 293": [
    "clean",
    0.5569361434413339
  ],
  "elementary_mathematics 316": [
    "clean",
    0.3194444444444444
  ],
  "elementary_mathematics 330": [
    "clean",
    0.36507936507936506
  ],
  "elementary_mathematics 331": [
    "input contamination",
    0.8695517500774371
  ],
  "elementary_mathematics 361": [
    "input-and-label contamination",
    0.9640728922378994
  ],
  "elementary_mathematics 366": [
    "clean",
    0.5917857142857144
  ],
  "elementary_mathematics 369": [
    "input-and-label contamination",
    0.9999460101500918
  ],
  "elementary_mathematics 373": [
    "input-and-label contamination",
    0.9723056389723057
  ],
  "elementary_mathematics 375": [
    "input-and-label contamination",
    0.9999915709974881
  ],
  "world_religions 6": [
    "clean",
    0.16666666666666666
  ],
  "world_religions 18": [
    "clean",
    0.07692307692307693
  ],
  "world_religions 61": [
    "clean",
    0.27875
  ],
  "world_religions 71": [
    "input-and-label contamination",
    0.9998177842565598
  ],
  "world_religions 78": [
    "input contamination",
    0.7114285714285714
  ],
  "world_religions 79": [
    "input-and-label contamination",
    0.7491111111111111
  ],
  "world_religions 89": [
    "clean",
    0.5111111111111111
  ],
  "world_religions 92": [
    "input contamination",
    0.9410615808823529
  ],
  "world_religions 131": [
    "clean",
    0.3716666666666667
  ],
  "world_religions 164": [
    "clean",
    0.16666666666666666
  ],
  "professional_medicine 4": [
    "clean",
    0.3117629033802547
  ],
  "professional_medicine 10": [
    "clean",
    0.3933714746994605
  ],
  "professional_medicine 42": [
    "clean",
    0.16304347826086957
  ],
  "professional_medicine 54": [
    "clean",
    0.061487268518518524
  ],
  "professional_medicine 90": [
    "clean",
    0
  ],
  "professional_medicine 108": [
    "clean",
    0.12483920761512733
  ],
  "professional_medicine 122": [
    "clean",
    0.2915234375
  ],
  "professional_medicine 137": [
    "clean",
    0.5107663647015315
  ],
  "professional_medicine 140": [
    "clean",
    0.10399728997289973
  ],
  "professional_medicine 165": [
    "input contamination",
    0.7454591019839887
  ],
  "professional_medicine 227": [
    "clean",
    0.09500475389453668
  ],
  "professional_medicine 248": [
    "input contamination",
    0.9830483282613515
  ],
  "professional_medicine 268": [
    "clean",
    0.056881950821344764
  ],
  "anatomy 7": [
    "input-and-label contamination",
    0.9017693014705882
  ],
  "anatomy 35": [
    "input-and-label contamination",
    0.9638310185185185
  ],
  "anatomy 65": [
    "input-and-label contamination",
    0.8813842997516467
  ],
  "anatomy 66": [
    "input-and-label contamination",
    0.9999530428249437
  ],
  "anatomy 67": [
    "clean",
    0.3931623931623932
  ],
  "anatomy 72": [
    "clean",
    0.576923076923077
  ],
  "anatomy 94": [
    "input-and-label contamination",
    0.8518518518518519
  ],
  "anatomy 104": [
    "input-and-label contamination",
    0.9363888888888889
  ]
}